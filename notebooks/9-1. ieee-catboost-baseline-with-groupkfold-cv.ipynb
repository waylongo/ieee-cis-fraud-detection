{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime, psutil\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "########################### Model\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, cat_params, NFOLDS=2, kfold_mode='grouped'):\n",
    "    \n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "    split_groups = tr_df['DT_M']\n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]] \n",
    "    tr_df = tr_df[['TransactionID',target]] \n",
    "    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    oof = np.zeros(len(tr_df))\n",
    "\n",
    "    if kfold_mode=='grouped':\n",
    "        folds = GroupKFold(n_splits=NFOLDS)\n",
    "        folds_split = folds.split(X, y, groups=split_groups)\n",
    "    else:\n",
    "        folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "        folds_split = folds.split(X, y)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds_split):        \n",
    "        print('Fold:',fold_)\n",
    "        \n",
    "        estimator = CatBoostClassifier(**cat_params)        \n",
    "        estimator.fit(\n",
    "            X.iloc[trn_idx,:],y[trn_idx],\n",
    "            eval_set=(X.iloc[val_idx,:], y[val_idx]),\n",
    "            cat_features=categorical_features,\n",
    "            use_best_model=True,\n",
    "            verbose=True)\n",
    "        \n",
    "        pp_p = estimator.predict_proba(P)[:,1]\n",
    "        predictions += pp_p/NFOLDS\n",
    "        \n",
    "        oof_preds = estimator.predict_proba(X.iloc[val_idx,:])[:,1]\n",
    "        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n",
    "        \n",
    "        del estimator\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n",
    "    if LOCAL_TEST:\n",
    "        print('Holdout AUC:', metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction']))\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "cat_params = {\n",
    "                'n_estimators':5000,\n",
    "                'learning_rate': 0.07,\n",
    "                'eval_metric':'AUC',\n",
    "                'loss_function':'Logloss',\n",
    "                'random_seed':SEED,\n",
    "                'metric_period':500,\n",
    "                'od_wait':500,\n",
    "                'task_type':'GPU',\n",
    "                'depth': 8,\n",
    "                #'colsample_bylevel':0.7,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Shape control: (590540, 791) (506691, 791)\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    train_df = pd.read_pickle('../input/ieee-fe-for-local-test/train_df.pkl')\n",
    "    test_df = pd.read_pickle('../input/ieee-fe-for-local-test/test_df.pkl')\n",
    "else:\n",
    "    train_df = pd.read_pickle('../input/ieee-fe-with-some-eda/train_df.pkl')\n",
    "    test_df = pd.read_pickle('../input/ieee-fe-with-some-eda/test_df.pkl')\n",
    "    \n",
    "remove_features = pd.read_pickle('../input/ieee-fe-with-some-eda/remove_features.pkl')\n",
    "remove_features = list(remove_features['features_to_remove'].values)\n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Encode NaN goups\n",
    "nans_groups = {}\n",
    "temp_df = train_df.isna()\n",
    "temp_df2 = test_df.isna()\n",
    "nans_df = pd.concat([temp_df, temp_df2])\n",
    "\n",
    "for col in list(nans_df):\n",
    "    cur_group = nans_df[col].sum()\n",
    "    if cur_group>0:\n",
    "        try:\n",
    "            nans_groups[cur_group].append(col)\n",
    "        except:\n",
    "            nans_groups[cur_group]=[col]\n",
    "\n",
    "add_category = []\n",
    "for col in nans_groups:\n",
    "    if len(nans_groups[col])>1:\n",
    "        train_df['nan_group_'+str(col)] = np.where(temp_df[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n",
    "        test_df['nan_group_'+str(col)]  = np.where(temp_df2[nans_groups[col]].sum(axis=1)>0,1,0).astype(np.int8)\n",
    "        add_category.append('nan_group_'+str(col))\n",
    "        \n",
    "del temp_df, temp_df2, nans_df, nans_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No transformation (keep only categorical) card4\n",
      "No transformation (keep only categorical) addr1\n",
      "No transformation (keep only categorical) addr2\n",
      "No transformation (keep only categorical) dist1\n",
      "No transformation (keep only categorical) dist2\n"
     ]
    }
   ],
   "source": [
    "########################### Copy original Categorical features\n",
    "categorical_features = ['ProductCD','M4',\n",
    "                        'card1','card2','card3','card4','card5','card6',\n",
    "                        'addr1','addr2','dist1','dist2',\n",
    "                        'P_emaildomain','R_emaildomain',\n",
    "                       ]\n",
    "\n",
    "o_trans = pd.concat([pd.read_pickle('../input/ieee-data-minification/train_transaction.pkl'),\n",
    "                     pd.read_pickle('../input/ieee-data-minification/test_transaction.pkl')])\n",
    "\n",
    "o_ident = pd.concat([pd.read_pickle('../input/ieee-data-minification/train_identity.pkl'),\n",
    "                     pd.read_pickle('../input/ieee-data-minification/test_identity.pkl')])\n",
    "\n",
    "o_trans = o_trans.merge(o_ident, on=['TransactionID'], how='left')\n",
    "o_trans = o_trans[['TransactionID'] + categorical_features]\n",
    "o_features = categorical_features.copy()\n",
    "categorical_features = [col+'_cat' for col in categorical_features]\n",
    "o_trans.columns = ['TransactionID'] + categorical_features\n",
    "del o_ident\n",
    "\n",
    "temp_df = train_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(o_trans, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(o_trans, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "del temp_df, o_trans\n",
    "\n",
    "for col in o_features:\n",
    "    if train_df[col].equals(train_df[col+'_cat']):\n",
    "        print('No transformation (keep only categorical)', col)\n",
    "        del train_df[col], test_df[col]\n",
    "        \n",
    "    col = col+'_cat'    \n",
    "    train_df[col] = train_df[col].fillna(-999)\n",
    "    test_df[col]  = test_df[col].fillna(-999)\n",
    "\n",
    "categorical_features += add_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: card3  | Dominator: 150.0\n",
      "Column: C3  | Dominator: 0.0\n",
      "Column: C7  | Dominator: 0.0\n",
      "Column: D6  | Dominator: 899261\n",
      "Column: D7  | Dominator: 998181\n",
      "Column: D8  | Dominator: 947967\n",
      "Column: D9  | Dominator: 947967\n",
      "Column: D12  | Dominator: 963260\n",
      "Column: D13  | Dominator: 911895\n",
      "Column: D14  | Dominator: 919850\n",
      "Column: V14  | Dominator: 1.0\n",
      "Column: V27  | Dominator: 0.0\n",
      "Column: V28  | Dominator: 0.0\n",
      "Column: V65  | Dominator: 1.0\n",
      "Column: V68  | Dominator: 0.0\n",
      "Column: V95  | Dominator: 0.0\n",
      "Column: V98  | Dominator: 0.0\n",
      "Column: V101  | Dominator: 0.0\n",
      "Column: V102  | Dominator: 0.0\n",
      "Column: V103  | Dominator: 0.0\n",
      "Column: V104  | Dominator: 0.0\n",
      "Column: V105  | Dominator: 0.0\n",
      "Column: V106  | Dominator: 0.0\n",
      "Column: V107  | Dominator: 1.0\n",
      "Column: V108  | Dominator: 1.0\n",
      "Column: V109  | Dominator: 1.0\n",
      "Column: V110  | Dominator: 1.0\n",
      "Column: V111  | Dominator: 1.0\n",
      "Column: V112  | Dominator: 1.0\n",
      "Column: V113  | Dominator: 1.0\n",
      "Column: V114  | Dominator: 1.0\n",
      "Column: V115  | Dominator: 1.0\n",
      "Column: V116  | Dominator: 1.0\n",
      "Column: V117  | Dominator: 1.0\n",
      "Column: V118  | Dominator: 1.0\n",
      "Column: V119  | Dominator: 1.0\n",
      "Column: V120  | Dominator: 1.0\n",
      "Column: V121  | Dominator: 1.0\n",
      "Column: V122  | Dominator: 1.0\n",
      "Column: V123  | Dominator: 1.0\n",
      "Column: V124  | Dominator: 1.0\n",
      "Column: V125  | Dominator: 1.0\n",
      "Column: V126  | Dominator: 0.0\n",
      "Column: V129  | Dominator: 0.0\n",
      "Column: V132  | Dominator: 0.0\n",
      "Column: V133  | Dominator: 0.0\n",
      "Column: V134  | Dominator: 0.0\n",
      "Column: V135  | Dominator: 0.0\n",
      "Column: V136  | Dominator: 0.0\n",
      "Column: V137  | Dominator: 0.0\n",
      "Column: V138  | Dominator: -999.0\n",
      "Column: V139  | Dominator: -999.0\n",
      "Column: V140  | Dominator: -999.0\n",
      "Column: V141  | Dominator: -999.0\n",
      "Column: V142  | Dominator: -999.0\n",
      "Column: V143  | Dominator: -999.0\n",
      "Column: V144  | Dominator: -999.0\n",
      "Column: V145  | Dominator: -999.0\n",
      "Column: V146  | Dominator: -999.0\n",
      "Column: V147  | Dominator: -999.0\n",
      "Column: V148  | Dominator: -999.0\n",
      "Column: V149  | Dominator: -999.0\n",
      "Column: V150  | Dominator: -999.0\n",
      "Column: V151  | Dominator: -999.0\n",
      "Column: V152  | Dominator: -999.0\n",
      "Column: V153  | Dominator: -999.0\n",
      "Column: V154  | Dominator: -999.0\n",
      "Column: V155  | Dominator: -999.0\n",
      "Column: V156  | Dominator: -999.0\n",
      "Column: V157  | Dominator: -999.0\n",
      "Column: V158  | Dominator: -999.0\n",
      "Column: V159  | Dominator: -999.0\n",
      "Column: V160  | Dominator: -999.0\n",
      "Column: V161  | Dominator: -999.0\n",
      "Column: V162  | Dominator: -999.0\n",
      "Column: V163  | Dominator: -999.0\n",
      "Column: V164  | Dominator: -999.0\n",
      "Column: V165  | Dominator: -999.0\n",
      "Column: V166  | Dominator: -999.0\n",
      "Column: V281  | Dominator: 0.0\n",
      "Column: V284  | Dominator: 0.0\n",
      "Column: V286  | Dominator: 0.0\n",
      "Column: V290  | Dominator: 1.0\n",
      "Column: V292  | Dominator: 1.0\n",
      "Column: V293  | Dominator: 0.0\n",
      "Column: V294  | Dominator: 0.0\n",
      "Column: V295  | Dominator: 0.0\n",
      "Column: V296  | Dominator: 0.0\n",
      "Column: V297  | Dominator: 0.0\n",
      "Column: V298  | Dominator: 0.0\n",
      "Column: V299  | Dominator: 0.0\n",
      "Column: V300  | Dominator: 0.0\n",
      "Column: V301  | Dominator: 0.0\n",
      "Column: V305  | Dominator: 1.0\n",
      "Column: V309  | Dominator: 0.0\n",
      "Column: V311  | Dominator: 0.0\n",
      "Column: V316  | Dominator: 0.0\n",
      "Column: V317  | Dominator: 0.0\n",
      "Column: V318  | Dominator: 0.0\n",
      "Column: V319  | Dominator: 0.0\n",
      "Column: V320  | Dominator: 0.0\n",
      "Column: V321  | Dominator: 0.0\n",
      "Column: V322  | Dominator: -999.0\n",
      "Column: V323  | Dominator: -999.0\n",
      "Column: V324  | Dominator: -999.0\n",
      "Column: V325  | Dominator: -999.0\n",
      "Column: V326  | Dominator: -999.0\n",
      "Column: V327  | Dominator: -999.0\n",
      "Column: V328  | Dominator: -999.0\n",
      "Column: V329  | Dominator: -999.0\n",
      "Column: V330  | Dominator: -999.0\n",
      "Column: V331  | Dominator: -999.0\n",
      "Column: V332  | Dominator: -999.0\n",
      "Column: V333  | Dominator: -999.0\n",
      "Column: V334  | Dominator: -999.0\n",
      "Column: V335  | Dominator: -999.0\n",
      "Column: V336  | Dominator: -999.0\n",
      "Column: V337  | Dominator: -999.0\n",
      "Column: V338  | Dominator: -999.0\n",
      "Column: V339  | Dominator: -999.0\n",
      "Column: is_holiday  | Dominator: 0\n",
      "Column: card3_fq_enc  | Dominator: 956845\n",
      "Column: uid5_D7_std  | Dominator: -999.0\n",
      "Column: uid3_D12_mean  | Dominator: -999.0\n",
      "Column: uid3_D12_std  | Dominator: -999.0\n",
      "Column: uid4_D12_mean  | Dominator: -999.0\n",
      "Column: uid4_D12_std  | Dominator: -999.0\n",
      "Column: uid5_D12_mean  | Dominator: -999.0\n",
      "Column: uid5_D12_std  | Dominator: -999.0\n",
      "Column: D9_not_na  | Dominator: 0\n",
      "Column: D8_not_same_day  | Dominator: 0\n",
      "Column: D8_D9_decimal_dist  | Dominator: -999.0\n",
      "Column: D6_DT_D_min_max  | Dominator: -999.0\n",
      "Column: D6_DT_D_std_score  | Dominator: -999.0\n",
      "Column: D7_DT_D_min_max  | Dominator: -999.0\n",
      "Column: D7_DT_D_std_score  | Dominator: -999.0\n",
      "Column: D8_DT_D_min_max  | Dominator: 0.0\n",
      "Column: D12_DT_D_min_max  | Dominator: -999.0\n",
      "Column: D12_DT_D_std_score  | Dominator: -999.0\n",
      "Column: D13_DT_D_min_max  | Dominator: -999.0\n",
      "Column: D13_DT_D_std_score  | Dominator: -999.0\n",
      "Column: D14_DT_D_min_max  | Dominator: -999.0\n",
      "Column: D14_DT_D_std_score  | Dominator: -999.0\n",
      "Column: D6_DT_W_min_max  | Dominator: -999.0\n",
      "Column: D6_DT_W_std_score  | Dominator: -999.0\n",
      "Column: D7_DT_W_min_max  | Dominator: -999.0\n",
      "Column: D7_DT_W_std_score  | Dominator: -999.0\n",
      "Column: D8_DT_W_min_max  | Dominator: 0.0\n",
      "Column: D12_DT_W_min_max  | Dominator: -999.0\n",
      "Column: D12_DT_W_std_score  | Dominator: -999.0\n",
      "Column: D13_DT_W_min_max  | Dominator: -999.0\n",
      "Column: D13_DT_W_std_score  | Dominator: -999.0\n",
      "Column: D14_DT_W_min_max  | Dominator: -999.0\n",
      "Column: D14_DT_W_std_score  | Dominator: -999.0\n",
      "Column: D6_DT_M_min_max  | Dominator: -999.0\n",
      "Column: D6_DT_M_std_score  | Dominator: -999.0\n",
      "Column: D7_DT_M_min_max  | Dominator: -999.0\n",
      "Column: D7_DT_M_std_score  | Dominator: -999.0\n",
      "Column: D8_DT_M_min_max  | Dominator: 0.0\n",
      "Column: D12_DT_M_min_max  | Dominator: -999.0\n",
      "Column: D12_DT_M_std_score  | Dominator: -999.0\n",
      "Column: D13_DT_M_min_max  | Dominator: -999.0\n",
      "Column: D13_DT_M_std_score  | Dominator: -999.0\n",
      "Column: D14_DT_M_min_max  | Dominator: -999.0\n",
      "Column: D14_DT_M_std_score  | Dominator: -999.0\n",
      "Column: TransactionAmt_check  | Dominator: 1\n",
      "Column: card3_TransactionAmt_mean  | Dominator: 147.52035522460938\n",
      "Column: card3_TransactionAmt_std  | Dominator: 249.32469177246094\n",
      "Column: C7_fq_enc  | Dominator: 961237\n",
      "Column: id_03  | Dominator: -999.0\n",
      "Column: id_04  | Dominator: -999.0\n",
      "Column: id_07  | Dominator: -999.0\n",
      "Column: id_08  | Dominator: -999.0\n",
      "Column: id_09  | Dominator: -999.0\n",
      "Column: id_10  | Dominator: -999.0\n",
      "Column: id_14  | Dominator: -999.0\n",
      "Column: id_18  | Dominator: -999.0\n",
      "Column: id_21  | Dominator: -999.0\n",
      "Column: id_22  | Dominator: -999.0\n",
      "Column: id_23  | Dominator: -999.0\n",
      "Column: id_24  | Dominator: -999.0\n",
      "Column: id_25  | Dominator: -999.0\n",
      "Column: id_26  | Dominator: -999.0\n",
      "Column: id_27  | Dominator: -999.0\n",
      "Column: id_32  | Dominator: -999.0\n",
      "Column: id_34  | Dominator: -999.0\n",
      "Column: nan_group_3  | Dominator: 0\n",
      "Column: nan_group_88567  | Dominator: 0\n",
      "Column: nan_group_963260  | Dominator: 1\n",
      "Column: nan_group_911895  | Dominator: 1\n",
      "Column: nan_group_919850  | Dominator: 1\n",
      "Column: nan_group_7300  | Dominator: 0\n",
      "Column: nan_group_899261  | Dominator: 1\n",
      "Column: nan_group_998181  | Dominator: 1\n",
      "Column: nan_group_947967  | Dominator: 1\n",
      "Column: nan_group_314  | Dominator: 0\n",
      "Column: nan_group_88662  | Dominator: 0\n",
      "Column: nan_group_939501  | Dominator: 1\n",
      "Column: nan_group_939225  | Dominator: 1\n",
      "Column: nan_group_15  | Dominator: 0\n",
      "Column: nan_group_938449  | Dominator: 1\n",
      "Column: nan_group_89995  | Dominator: 0\n",
      "Column: nan_group_131315  | Dominator: 0\n",
      "Column: nan_group_447  | Dominator: 0\n",
      "Column: nan_group_870  | Dominator: 0\n",
      "Column: nan_group_20934  | Dominator: 0\n",
      "Column: nan_group_22739  | Dominator: 0\n",
      "Column: nan_group_4660  | Dominator: 0\n",
      "Column: nan_group_16139  | Dominator: 0\n",
      "Column: nan_group_964426  | Dominator: 1\n",
      "Column: nan_group_1087017  | Dominator: 1\n",
      "Column: nan_group_1087000  | Dominator: 1\n",
      "Column: nan_group_99095  | Dominator: 0\n",
      "Column: nan_group_143363  | Dominator: 0\n",
      "Column: nan_group_81789  | Dominator: 0\n",
      "Column: nan_group_126140  | Dominator: 0\n",
      "Column: card3_cat  | Dominator: 150.0\n",
      "Column: addr2_cat  | Dominator: 87.0\n",
      "Column: dist2_cat  | Dominator: -999.0\n"
     ]
    }
   ],
   "source": [
    "########################### Transform Heavy Dominated columns\n",
    "total_items = len(train_df)\n",
    "keep_cols = [TARGET,'C3_fq_enc']\n",
    "\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype.name!='category':\n",
    "        cur_dominator = list(train_df[col].fillna(-999).value_counts())[0]\n",
    "        if (cur_dominator/total_items > 0.85) and (col not in keep_cols):\n",
    "            cur_dominator = train_df[col].fillna(-999).value_counts().index[0]\n",
    "            print('Column:', col, ' | Dominator:', cur_dominator)\n",
    "            train_df[col] = np.where(train_df[col].fillna(-999)==cur_dominator,1,0)\n",
    "            test_df[col] = np.where(test_df[col].fillna(-999)==cur_dominator,1,0)\n",
    "\n",
    "            train_df[col] = train_df[col].fillna(-999).astype(int)\n",
    "            test_df[col] = test_df[col].fillna(-999).astype(int)\n",
    "\n",
    "            if col not in categorical_features:\n",
    "                categorical_features.append(col)\n",
    "                \n",
    "categorical_features +=['D8_not_same_day','TransactionAmt_check']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Restore some categorical features\n",
    "## These features weren't useful for lgbm\n",
    "## but catboost can use it\n",
    "restore_features = [\n",
    "                    'uid','uid2','uid3','uid4','uid5','bank_type',\n",
    "                    ]\n",
    "\n",
    "for col in restore_features:\n",
    "    categorical_features.append(col)\n",
    "    remove_features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate card3_fq_enc\n",
      "Duplicate card3_TransactionAmt_mean\n",
      "Duplicate card3_TransactionAmt_std\n",
      "Duplicate card3_cat\n",
      "Duplicate C7_fq_enc\n",
      "Duplicate D6_DT_D_min_max\n",
      "Duplicate D6_DT_D_std_score\n",
      "Duplicate D6_DT_W_min_max\n",
      "Duplicate D6_DT_W_std_score\n",
      "Duplicate D6_DT_M_min_max\n",
      "Duplicate D6_DT_M_std_score\n",
      "Duplicate nan_group_899261\n",
      "Duplicate D7_DT_D_min_max\n",
      "Duplicate D7_DT_D_std_score\n",
      "Duplicate D7_DT_W_min_max\n",
      "Duplicate D7_DT_W_std_score\n",
      "Duplicate D7_DT_M_min_max\n",
      "Duplicate D7_DT_M_std_score\n",
      "Duplicate nan_group_998181\n",
      "Duplicate D9\n",
      "Duplicate D9_not_na\n",
      "Duplicate D8_D9_decimal_dist\n",
      "Duplicate D8_DT_D_min_max\n",
      "Duplicate D8_DT_W_min_max\n",
      "Duplicate D8_DT_M_min_max\n",
      "Duplicate id_09\n",
      "Duplicate id_10\n",
      "Duplicate nan_group_947967\n",
      "Duplicate D12_DT_D_min_max\n",
      "Duplicate D12_DT_D_std_score\n",
      "Duplicate D12_DT_W_min_max\n",
      "Duplicate D12_DT_W_std_score\n",
      "Duplicate D12_DT_M_min_max\n",
      "Duplicate D12_DT_M_std_score\n",
      "Duplicate nan_group_963260\n",
      "Duplicate D13_DT_D_min_max\n",
      "Duplicate D13_DT_D_std_score\n",
      "Duplicate D13_DT_W_min_max\n",
      "Duplicate D13_DT_W_std_score\n",
      "Duplicate D13_DT_M_min_max\n",
      "Duplicate D13_DT_M_std_score\n",
      "Duplicate nan_group_911895\n",
      "Duplicate D14_DT_D_min_max\n",
      "Duplicate D14_DT_D_std_score\n",
      "Duplicate D14_DT_W_min_max\n",
      "Duplicate D14_DT_W_std_score\n",
      "Duplicate D14_DT_M_min_max\n",
      "Duplicate D14_DT_M_std_score\n",
      "Duplicate nan_group_919850\n",
      "Duplicate V126\n",
      "Duplicate V129\n",
      "Duplicate V132\n",
      "Duplicate V133\n",
      "Duplicate V134\n",
      "Duplicate V135\n",
      "Duplicate V136\n",
      "Duplicate V137\n",
      "Duplicate V139\n",
      "Duplicate V140\n",
      "Duplicate V141\n",
      "Duplicate V142\n",
      "Duplicate V146\n",
      "Duplicate V147\n",
      "Duplicate V148\n",
      "Duplicate V149\n",
      "Duplicate V153\n",
      "Duplicate V154\n",
      "Duplicate V155\n",
      "Duplicate V156\n",
      "Duplicate V157\n",
      "Duplicate V158\n",
      "Duplicate V161\n",
      "Duplicate V162\n",
      "Duplicate V163\n",
      "Duplicate nan_group_939501\n",
      "Duplicate V144\n",
      "Duplicate V145\n",
      "Duplicate V150\n",
      "Duplicate V151\n",
      "Duplicate V152\n",
      "Duplicate V159\n",
      "Duplicate V160\n",
      "Duplicate V164\n",
      "Duplicate V165\n",
      "Duplicate V166\n",
      "Duplicate nan_group_939225\n",
      "Duplicate V309\n",
      "Duplicate V311\n",
      "Duplicate V316\n",
      "Duplicate V317\n",
      "Duplicate V318\n",
      "Duplicate V319\n",
      "Duplicate V320\n",
      "Duplicate V321\n",
      "Duplicate V323\n",
      "Duplicate V324\n",
      "Duplicate V325\n",
      "Duplicate V326\n",
      "Duplicate V327\n",
      "Duplicate V328\n",
      "Duplicate V329\n",
      "Duplicate V330\n",
      "Duplicate V331\n",
      "Duplicate V332\n",
      "Duplicate V333\n",
      "Duplicate V334\n",
      "Duplicate V335\n",
      "Duplicate V336\n",
      "Duplicate V337\n",
      "Duplicate V338\n",
      "Duplicate V339\n",
      "Duplicate nan_group_938449\n",
      "Duplicate id_04\n",
      "Duplicate nan_group_964426\n",
      "Duplicate id_08\n",
      "Duplicate nan_group_1087017\n",
      "Duplicate id_27\n",
      "Duplicate nan_group_1087000\n"
     ]
    }
   ],
   "source": [
    "########################### Remove 100% duplicated columns\n",
    "cols_sum = {}\n",
    "bad_types = ['datetime64[ns]', 'category','object']\n",
    "\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype.name not in bad_types:\n",
    "        cur_col = train_df[col].values\n",
    "        cur_sum = cur_col.mean()\n",
    "        try:\n",
    "            cols_sum[cur_sum].append(col)\n",
    "        except:\n",
    "            cols_sum[cur_sum] = [col]\n",
    "\n",
    "cols_sum = {k:v for k,v in cols_sum.items() if len(v)>1}   \n",
    "\n",
    "for k,v in cols_sum.items():\n",
    "    for col in v[1:]:\n",
    "        if train_df[v[0]].equals(train_df[col]):\n",
    "            print('Duplicate', col)\n",
    "            del train_df[col], test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_emaildomain_cat\n",
      "R_emaildomain_cat\n"
     ]
    }
   ],
   "source": [
    "########################### Encode Str columns\n",
    "# As we restored some original features\n",
    "# we nned to run LabelEncoder to reduce\n",
    "# memory usage and garant that there are no nans\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "    \n",
    "    elif col in categorical_features:\n",
    "        train_df[col] = train_df[col].astype(float).fillna(-999)\n",
    "        test_df[col]  = test_df[col].astype(float).fillna(-999)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1092.02 Mb (54.0% reduction)\n",
      "Mem. usage decreased to 943.73 Mb (53.8% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Final features list\n",
    "features_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "categorical_features = [col for col in categorical_features if col in features_columns]\n",
    "\n",
    "########################### Final Minification\n",
    "## I don't like this part as it changes float numbers\n",
    "## small change but change.\n",
    "## To be able to train catboost without \n",
    "## minification we need to do some changes on model\n",
    "## we will do it later.\n",
    "if not LOCAL_TEST:\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    test_df  = reduce_mem_usage(test_df)\n",
    "    \n",
    "train_df = train_df[['TransactionID','DT_M',TARGET]+features_columns]\n",
    "test_df  = test_df[['TransactionID','DT_M',TARGET]+features_columns]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      train_df:   1.1GiB\n",
      "                       test_df: 930.2MiB\n",
      "              features_columns:   6.1KiB\n",
      "                           _i2:   2.4KiB\n",
      "                           _i8:   1.7KiB\n",
      "                           _i3:   1.7KiB\n",
      "          categorical_features:   1.2KiB\n",
      "                      cols_sum:   1.2KiB\n",
      "                           _ii:   1.1KiB\n",
      "                          _i12:   1.1KiB\n",
      "Memory in Gb 4.21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Cleaning\n",
    "# Check what variables consume memory\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "print('Memory in Gb', get_memory_usage())\n",
    "\n",
    "# Confirm thar variable exist\n",
    "temp_df = 0\n",
    "\n",
    "del temp_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "0:\tlearn: 0.8613991\ttest: 0.7356347\tbest: 0.7356347 (0)\ttotal: 144ms\tremaining: 11m 57s\n",
      "500:\tlearn: 0.9709161\ttest: 0.9051092\tbest: 0.9051092 (500)\ttotal: 1m 4s\tremaining: 9m 35s\n",
      "1000:\tlearn: 0.9775985\ttest: 0.9116461\tbest: 0.9116461 (1000)\ttotal: 2m 7s\tremaining: 8m 27s\n",
      "1500:\tlearn: 0.9827214\ttest: 0.9151939\tbest: 0.9152679 (1487)\ttotal: 3m 10s\tremaining: 7m 24s\n",
      "2000:\tlearn: 0.9866871\ttest: 0.9154721\tbest: 0.9158686 (1749)\ttotal: 4m 14s\tremaining: 6m 21s\n",
      "2500:\tlearn: 0.9898605\ttest: 0.9164411\tbest: 0.9167305 (2472)\ttotal: 5m 18s\tremaining: 5m 17s\n",
      "3000:\tlearn: 0.9924441\ttest: 0.9172840\tbest: 0.9175852 (2967)\ttotal: 6m 22s\tremaining: 4m 14s\n",
      "bestTest = 0.9175851643\n",
      "bestIteration = 2967\n",
      "Shrink model to first 2968 iterations.\n",
      "Fold: 1\n",
      "0:\tlearn: 0.8663313\ttest: 0.8422300\tbest: 0.8422300 (0)\ttotal: 150ms\tremaining: 12m 30s\n",
      "500:\tlearn: 0.9680697\ttest: 0.9430394\tbest: 0.9430426 (498)\ttotal: 1m 5s\tremaining: 9m 47s\n",
      "1000:\tlearn: 0.9750525\ttest: 0.9443727\tbest: 0.9443772 (995)\ttotal: 2m 10s\tremaining: 8m 42s\n",
      "bestTest = 0.9443772435\n",
      "bestIteration = 995\n",
      "Shrink model to first 996 iterations.\n",
      "Fold: 2\n",
      "0:\tlearn: 0.8684123\ttest: 0.8378400\tbest: 0.8378400 (0)\ttotal: 148ms\tremaining: 12m 20s\n",
      "500:\tlearn: 0.9670887\ttest: 0.9419233\tbest: 0.9419233 (500)\ttotal: 1m 6s\tremaining: 9m 54s\n",
      "1000:\tlearn: 0.9739603\ttest: 0.9431103\tbest: 0.9432598 (968)\ttotal: 2m 12s\tremaining: 8m 51s\n",
      "1500:\tlearn: 0.9789794\ttest: 0.9439093\tbest: 0.9439469 (1423)\ttotal: 3m 19s\tremaining: 7m 44s\n",
      "2000:\tlearn: 0.9834214\ttest: 0.9440758\tbest: 0.9443098 (1762)\ttotal: 4m 25s\tremaining: 6m 38s\n",
      "2500:\tlearn: 0.9873153\ttest: 0.9443995\tbest: 0.9446223 (2488)\ttotal: 5m 32s\tremaining: 5m 32s\n",
      "bestTest = 0.9446223378\n",
      "bestIteration = 2488\n",
      "Shrink model to first 2489 iterations.\n",
      "Fold: 3\n",
      "0:\tlearn: 0.8689704\ttest: 0.8311232\tbest: 0.8311232 (0)\ttotal: 151ms\tremaining: 12m 33s\n",
      "500:\tlearn: 0.9681872\ttest: 0.9330334\tbest: 0.9330334 (500)\ttotal: 1m 6s\tremaining: 9m 54s\n",
      "1000:\tlearn: 0.9747908\ttest: 0.9340037\tbest: 0.9342728 (716)\ttotal: 2m 13s\tremaining: 8m 51s\n",
      "bestTest = 0.9342728257\n",
      "bestIteration = 716\n",
      "Shrink model to first 717 iterations.\n",
      "Fold: 4\n",
      "0:\tlearn: 0.8511343\ttest: 0.8474253\tbest: 0.8474253 (0)\ttotal: 156ms\tremaining: 13m\n",
      "500:\tlearn: 0.9680842\ttest: 0.9444806\tbest: 0.9444905 (495)\ttotal: 1m 7s\tremaining: 10m 2s\n",
      "1000:\tlearn: 0.9748877\ttest: 0.9454730\tbest: 0.9455180 (990)\ttotal: 2m 13s\tremaining: 8m 54s\n",
      "1500:\tlearn: 0.9800830\ttest: 0.9451008\tbest: 0.9457362 (1125)\ttotal: 3m 21s\tremaining: 7m 48s\n",
      "bestTest = 0.9457361698\n",
      "bestIteration = 1125\n",
      "Shrink model to first 1126 iterations.\n",
      "Fold: 5\n",
      "0:\tlearn: 0.8690769\ttest: 0.8516439\tbest: 0.8516439 (0)\ttotal: 149ms\tremaining: 12m 25s\n",
      "500:\tlearn: 0.9679212\ttest: 0.9585362\tbest: 0.9585362 (500)\ttotal: 1m 7s\tremaining: 10m 4s\n",
      "1000:\tlearn: 0.9740997\ttest: 0.9605642\tbest: 0.9606313 (951)\ttotal: 2m 13s\tremaining: 8m 54s\n",
      "1500:\tlearn: 0.9793881\ttest: 0.9613982\tbest: 0.9615020 (1413)\ttotal: 3m 20s\tremaining: 7m 47s\n",
      "2000:\tlearn: 0.9836512\ttest: 0.9625208\tbest: 0.9625437 (1864)\ttotal: 4m 27s\tremaining: 6m 41s\n",
      "2500:\tlearn: 0.9872915\ttest: 0.9623678\tbest: 0.9627786 (2083)\ttotal: 5m 35s\tremaining: 5m 35s\n",
      "bestTest = 0.9627786279\n",
      "bestIteration = 2083\n",
      "Shrink model to first 2084 iterations.\n",
      "OOF AUC: 0.940600021229858\n"
     ]
    }
   ],
   "source": [
    "########################### Model Train\n",
    "if LOCAL_TEST:\n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, cat_params, \n",
    "                                        NFOLDS=4, kfold_mode='grouped')\n",
    "\n",
    "else:    \n",
    "    # Why NFOLDS = 6 -> we have 6 months -> let's split it by month))\n",
    "    NFOLDS = 6\n",
    "    folds = GroupKFold(n_splits=NFOLDS)\n",
    "\n",
    "    X,y = train_df[features_columns], train_df[TARGET]    \n",
    "    P,P_y = test_df[features_columns], test_df[TARGET]  \n",
    "    \n",
    "    split_groups = train_df['DT_M']\n",
    "    # We don't need original sets anymore\n",
    "    # let's reduce it\n",
    "    train_df = train_df[['TransactionID',TARGET]] \n",
    "    test_df = test_df[['TransactionID',TARGET]] \n",
    "    test_df['prediction'] = 0\n",
    "    gc.collect()\n",
    "    \n",
    "    oof = np.zeros(len(train_df))\n",
    "    predictions = np.zeros(len(test_df))\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('Fold:',fold_)\n",
    "        \n",
    "        estimator = CatBoostClassifier(**cat_params)        \n",
    "        estimator.fit(\n",
    "            X.iloc[trn_idx,:],y[trn_idx],\n",
    "            eval_set=(X.iloc[val_idx,:], y[val_idx]),\n",
    "            cat_features=categorical_features,\n",
    "            use_best_model=True,\n",
    "            verbose=True)\n",
    "\n",
    "        oof_preds = estimator.predict_proba(X.iloc[val_idx,:])[:,1]\n",
    "        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n",
    "        test_df['prediction'] += estimator.predict_proba(P)[:,1]/NFOLDS\n",
    "        \n",
    "        del estimator\n",
    "        gc.collect()\n",
    "        \n",
    "    print('OOF AUC:', metrics.roc_auc_score(y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_df['isFraud'] = test_df['prediction']\n",
    "    test_df[['TransactionID','isFraud']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

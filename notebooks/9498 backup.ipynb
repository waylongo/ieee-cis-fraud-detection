{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=5):\n",
    "    \n",
    "    folds = GroupKFold(n_splits=NFOLDS)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "    split_groups = tr_df['DT_M']\n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    oof = np.zeros(len(tr_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "        \n",
    "        oof_preds = estimator.predict(vl_x)\n",
    "        oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n",
    "\n",
    "#         if LOCAL_TEST:\n",
    "        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "        print(feature_imp)\n",
    "\n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n",
    "    if LOCAL_TEST:\n",
    "        print('Holdout AUC:', metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction']))\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.5,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Shape control: (590540, 791) (506691, 791)\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    train_df = pd.read_pickle('../input/ieee-fe-for-local-test/train_df.pkl')\n",
    "    test_df = pd.read_pickle('../input/ieee-fe-for-local-test/test_df.pkl') \n",
    "else:\n",
    "    train_df = pd.read_pickle('../input/ieee-fe-with-some-eda/train_df.pkl')\n",
    "    test_df = pd.read_pickle('../input/ieee-fe-with-some-eda/test_df.pkl')\n",
    "    \n",
    "remove_features = pd.read_pickle('../input/ieee-fe-with-some-eda/remove_features.pkl')\n",
    "remove_features = list(remove_features['features_to_remove'].values)\n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe with amount (user id)\n",
    "def fe_uid(train_df, test_df, rm_features):\n",
    "    \n",
    "    print('==> processing uid...')\n",
    "    \n",
    "    # user id, save for later aggregation\n",
    "    for df in [train_df, test_df]:\n",
    "        df['cents'] = df['TransactionAmt'].apply(lambda x: x - int(x))\n",
    "        df['uid_0'] = df['cents'].astype(str)+'_'+df['ProductCD'].astype(str)\n",
    "        df['uid_1'] = df['ProductCD'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)+'_'+df['id_19'].astype(str)+'_'+df['id_20'].astype(str)\n",
    "        df['uid_2'] = df['ProductCD'].astype(str)+'_'+df['card1'].astype(str)+'_'+df['card2'].astype(str)\n",
    "        df['uid_3'] = df['ProductCD'].astype(str)+'_'+df['card1'].astype(str)+'_'+df['card2'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)\n",
    "        df['uid_4'] = df['ProductCD'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "        df['uid_5'] = df['ProductCD'].astype(str)+'_'+df['card1'].astype(str)+'_'+df['card2'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "        df['uid_6'] = df['ProductCD'].astype(str)+'_'+df['P_emaildomain'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n",
    "        df['uid_7'] = df['ProductCD'].astype(str)+'_'+df['card1'].astype(str)+'_'+df['card2'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)+'_'+df['P_emaildomain'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n",
    "        df['uid_8'] = df['ProductCD'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)+'_'+df['P_emaildomain'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n",
    "        df['uid_9'] = df['ProductCD'].astype(str)+'_'+df['card1'].astype(str)+'_'+df['card2'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)+'_'+df['P_emaildomain'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n",
    "        \n",
    "        df['uid_10'] = df['uid_9'].astype(str)+'_'+df['id_19'].astype(str)+'_'+df['id_20'].astype(str)\n",
    "        df['uid_11'] = df['uid_9'].astype(str)+'_'+df['C1'].astype(str)+'_'+df['C2'].astype(str)\n",
    "        df['uid_12'] = df['uid_9'].astype(str)+'_'+df['D1'].astype(str)+'_'+df['D2'].astype(str)\n",
    "        df['uid_13'] = df['uid_9'].astype(str)+'_'+df['dist1'].astype(str)+'_'+df['dist2'].astype(str)\n",
    "        df['uid_13'] = df['uid_9'].astype(str)+'_'+df['D10'].astype(str)+'_'+df['D11'].astype(str)+'_'+df['D15'].astype(str)\n",
    "        df['uid_14'] = df['uid_9'].astype(str)+'_'+df['C5'].astype(str)+'_'+df['C6'].astype(str)\n",
    "        df['uid_15'] = df['uid_9'].astype(str)+'_'+df['id_30'].astype(str)+'_'+df['id_31'].astype(str)\n",
    "        df['uid_16'] = df['uid_9'].astype(str)+'_'+df['C5'].astype(str)+'_'+df['C6'].astype(str)\n",
    "        df['uid_17'] = df['uid_9'].astype(str)+'_'+df['C9'].astype(str)+'_'+df['C11'].astype(str)\n",
    "        df['uid_18'] = df['uid_9'].astype(str)+'_'+df['C13'].astype(str)+'_'+df['C14'].astype(str)\n",
    "        df['uid_19'] = df['uid_9'].astype(str)+'_'+df['D3'].astype(str)+'_'+df['D4'].astype(str)+'_'+df['D5'].astype(str)\n",
    "        df['uid_20'] = df['uid_9'].astype(str)+'_'+df['dist1'].astype(str)\n",
    "        df['uid_21'] = df['uid_9'].astype(str)+'_'+df['dist2'].astype(str)\n",
    "        df['uid_22'] = df['uid_9'].astype(str)+'_'+df['M1'].astype(str)+'_'+df['M2'].astype(str)+'_'+df['M3'].astype(str)+'_'+df['M4'].astype(str)+'_'+df['M5'].astype(str)+'_'+df['M6'].astype(str)+'_'+df['M7'].astype(str)+'_'+df['M8'].astype(str)+'_'+df['M9'].astype(str)\n",
    "        df['uid_23'] = df['uid_9'].astype(str)+'_'+df['V129'].astype(str)+'_'+df['V130'].astype(str)+'_'+df['V131'].astype(str)\n",
    "        df['uid_24'] = df['uid_9'].astype(str)+'_'+df['V310'].astype(str)+'_'+df['V311'].astype(str)+'_'+df['V312'].astype(str)+'_'+df['V313'].astype(str)+'_'+df['V314'].astype(str)+'_'+df['V315'].astype(str)\n",
    "        df['uid_25'] = df['uid_9'].astype(str)+'_'+df['C4'].astype(str)+'_'+df['C6'].astype(str)+'_'+df['C8'].astype(str)+'_'+df['C10'].astype(str)\n",
    "        df['uid_26'] = df['uid_9'].astype(str)+'_'+df['D8'].astype(str)+'_'+df['D9'].astype(str)\n",
    "        df['uid_27'] = df['uid_9'].astype(str)+'_'+df['DeviceInfo'].astype(str)+'_'+df['DeviceType'].astype(str)\n",
    "        df['uid_28'] = df['uid_9'].astype(str)+'_'+df['id_01'].astype(str)+'_'+df['id_02'].astype(str)\n",
    "        df['uid_29'] = df['uid_9'].astype(str)+'_'+df['id_05'].astype(str)+'_'+df['id_06'].astype(str)+'_'+df['id_07'].astype(str)+'_'+df['id_08'].astype(str)\n",
    "        df['uid_30'] = df['uid_9'].astype(str)+'_'+df['id_13'].astype(str)+'_'+df['id_14'].astype(str)+'_'+df['id_15'].astype(str)+'_'+df['id_16'].astype(str)\n",
    "        df['uid_31'] = df['uid_9'].astype(str)+'_'+df['id_30'].astype(str)+'_'+df['id_31'].astype(str)+'_'+df['id_33'].astype(str)\n",
    "        df['uid_32'] = df['uid_9'].astype(str)+'_'+df['TransactionAmt'].astype(str)\n",
    "        df['uid_33'] = df['uid_9'].astype(str)+'_'+df['id_09'].astype(str)+'_'+df['id_10'].astype(str)+'_'+df['id_11'].astype(str)\n",
    "        df['uid_34'] = df['uid_9'].astype(str)+'_'+df['DT_M'].astype(str)\n",
    "        df['uid_35'] = df['uid_9'].astype(str)+'_'+df['DT_W'].astype(str)\n",
    "        df['uid_36'] = df['uid_9'].astype(str)+'_'+df['DT_D'].astype(str)\n",
    "        df['uid_37'] = df['uid_9'].astype(str)+'_'+df['DT_hour'].astype(str)\n",
    "        df['uid_38'] = df['uid_9'].astype(str)+'_'+df['DT_day_week'].astype(str)\n",
    "        df['uid_39'] = df['uid_9'].astype(str)+'_'+df['DT_day_month'].astype(str)\n",
    "\n",
    "        df['uid_40'] = df['uid_8'].astype(str)+'_'+df['id_19'].astype(str)+'_'+df['id_20'].astype(str)\n",
    "        df['uid_41'] = df['uid_8'].astype(str)+'_'+df['C1'].astype(str)+'_'+df['C2'].astype(str)\n",
    "        df['uid_42'] = df['uid_8'].astype(str)+'_'+df['D1'].astype(str)+'_'+df['D2'].astype(str)\n",
    "        df['uid_43'] = df['uid_8'].astype(str)+'_'+df['dist1'].astype(str)+'_'+df['dist2'].astype(str)\n",
    "        df['uid_43'] = df['uid_8'].astype(str)+'_'+df['D10'].astype(str)+'_'+df['D11'].astype(str)+'_'+df['D15'].astype(str)\n",
    "        df['uid_44'] = df['uid_8'].astype(str)+'_'+df['C5'].astype(str)+'_'+df['C6'].astype(str)\n",
    "        df['uid_45'] = df['uid_8'].astype(str)+'_'+df['id_30'].astype(str)+'_'+df['id_31'].astype(str)\n",
    "        df['uid_46'] = df['uid_8'].astype(str)+'_'+df['C5'].astype(str)+'_'+df['C6'].astype(str)\n",
    "        df['uid_47'] = df['uid_8'].astype(str)+'_'+df['C9'].astype(str)+'_'+df['C11'].astype(str)\n",
    "        df['uid_48'] = df['uid_8'].astype(str)+'_'+df['C13'].astype(str)+'_'+df['C14'].astype(str)\n",
    "        df['uid_49'] = df['uid_8'].astype(str)+'_'+df['D3'].astype(str)+'_'+df['D4'].astype(str)+'_'+df['D5'].astype(str)\n",
    "        df['uid_50'] = df['uid_8'].astype(str)+'_'+df['dist1'].astype(str)\n",
    "        df['uid_51'] = df['uid_8'].astype(str)+'_'+df['dist2'].astype(str)\n",
    "        df['uid_52'] = df['uid_8'].astype(str)+'_'+df['M1'].astype(str)+'_'+df['M2'].astype(str)+'_'+df['M3'].astype(str)+'_'+df['M4'].astype(str)+'_'+df['M5'].astype(str)+'_'+df['M6'].astype(str)+'_'+df['M7'].astype(str)+'_'+df['M8'].astype(str)+'_'+df['M9'].astype(str)\n",
    "        df['uid_53'] = df['uid_8'].astype(str)+'_'+df['V129'].astype(str)+'_'+df['V130'].astype(str)+'_'+df['V131'].astype(str)\n",
    "        df['uid_54'] = df['uid_8'].astype(str)+'_'+df['V310'].astype(str)+'_'+df['V311'].astype(str)+'_'+df['V312'].astype(str)+'_'+df['V313'].astype(str)+'_'+df['V314'].astype(str)+'_'+df['V315'].astype(str)\n",
    "        df['uid_55'] = df['uid_8'].astype(str)+'_'+df['C4'].astype(str)+'_'+df['C6'].astype(str)+'_'+df['C8'].astype(str)+'_'+df['C10'].astype(str)\n",
    "        df['uid_56'] = df['uid_8'].astype(str)+'_'+df['D8'].astype(str)+'_'+df['D9'].astype(str)\n",
    "        df['uid_57'] = df['uid_8'].astype(str)+'_'+df['DeviceInfo'].astype(str)+'_'+df['DeviceType'].astype(str)\n",
    "        df['uid_58'] = df['uid_8'].astype(str)+'_'+df['id_01'].astype(str)+'_'+df['id_02'].astype(str)\n",
    "        df['uid_59'] = df['uid_8'].astype(str)+'_'+df['id_05'].astype(str)+'_'+df['id_06'].astype(str)+'_'+df['id_07'].astype(str)+'_'+df['id_08'].astype(str)\n",
    "        df['uid_60'] = df['uid_8'].astype(str)+'_'+df['id_13'].astype(str)+'_'+df['id_14'].astype(str)+'_'+df['id_15'].astype(str)+'_'+df['id_16'].astype(str)\n",
    "        df['uid_61'] = df['uid_8'].astype(str)+'_'+df['id_30'].astype(str)+'_'+df['id_31'].astype(str)+'_'+df['id_33'].astype(str)\n",
    "        df['uid_62'] = df['uid_8'].astype(str)+'_'+df['TransactionAmt'].astype(str)\n",
    "        df['uid_63'] = df['uid_8'].astype(str)+'_'+df['id_09'].astype(str)+'_'+df['id_10'].astype(str)+'_'+df['id_11'].astype(str)\n",
    "        df['uid_64'] = df['uid_8'].astype(str)+'_'+df['DT_M'].astype(str)\n",
    "        df['uid_65'] = df['uid_8'].astype(str)+'_'+df['DT_W'].astype(str)\n",
    "        df['uid_66'] = df['uid_8'].astype(str)+'_'+df['DT_D'].astype(str)\n",
    "        df['uid_67'] = df['uid_8'].astype(str)+'_'+df['DT_hour'].astype(str)\n",
    "        df['uid_68'] = df['uid_8'].astype(str)+'_'+df['DT_day_week'].astype(str)\n",
    "        df['uid_69'] = df['uid_8'].astype(str)+'_'+df['DT_day_month'].astype(str)\n",
    "        \n",
    "    uid_list = ['uid_' + str(i) for i in range(70)]\n",
    "    cid_list  =[]\n",
    "            \n",
    "    tmp_rm = [] + uid_list\n",
    "\n",
    "    rm_features = rm_features + tmp_rm\n",
    "    return train_df, test_df, rm_features, uid_list, cid_list\n",
    "# train, test, rm_features = fe_uid(train, test, rm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group aggregation\n",
    "def fe_agg(train_df, test_df, rm_features, uid_list, cid_list):\n",
    "\n",
    "    print('==> processing aggregation...')\n",
    "\n",
    "    uid_cols = uid_list\n",
    "    \n",
    "    for col in tqdm_notebook(uid_cols):\n",
    "        # aggr: mean, std, min, max, sum\n",
    "        for agg_type in ['mean', 'std']:\n",
    "            for agg_col in ['TransactionAmt']:\n",
    "                new_col_name = col + '_' + agg_col + '_' + agg_type\n",
    "                temp_df = pd.concat(\n",
    "                    [train_df[[col, agg_col]], test_df[[col, agg_col]]])\n",
    "                temp_df = temp_df.groupby([col])[agg_col].agg([\n",
    "                    agg_type\n",
    "                ]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name] = test_df[col].map(temp_df)  \n",
    "                \n",
    "#     for col in tqdm_notebook(uid_cols):\n",
    "#         # aggr: value - mean\n",
    "#         for agg_type in ['mean']:\n",
    "#             for agg_col in ['TransactionAmt']:\n",
    "#                 new_col_name = col + '_' + agg_col + '_' + agg_type + '_diff'\n",
    "#                 temp_df = pd.concat(\n",
    "#                     [train_df[[col, agg_col]], test_df[[col, agg_col]]])\n",
    "#                 temp_df = temp_df.groupby([col])[agg_col].agg([\n",
    "#                     agg_type\n",
    "#                 ]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "\n",
    "#                 temp_df.index = list(temp_df[col])\n",
    "#                 temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "#                 train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "#                 test_df[new_col_name] = test_df[col].map(temp_df) \n",
    "#                 train_df[new_col_name] = train_df[agg_col] - train_df[new_col_name]\n",
    "#                 test_df[new_col_name] = test_df[agg_col] - test_df[new_col_name]\n",
    "                \n",
    "    for col in tqdm_notebook(uid_cols + cid_list):\n",
    "        # count\n",
    "        for agg_type in ['count']:\n",
    "            for agg_col in ['TransactionDT']:\n",
    "                new_col_name = col + '_' + agg_type\n",
    "                temp_df = pd.concat(\n",
    "                    [train_df[[col, agg_col]], test_df[[col, agg_col]]])\n",
    "                temp_df = temp_df.groupby([col])[agg_col].agg([\n",
    "                    agg_type\n",
    "                ]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()\n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name] = test_df[col].map(temp_df)\n",
    "        \n",
    "        \n",
    "    train_df = train_df.replace(np.inf, 999)\n",
    "    test_df = test_df.replace(np.inf, 999)\n",
    "    \n",
    "    tmp_rm = []\n",
    "    rm_features = rm_features + tmp_rm\n",
    "    return train_df, test_df, rm_features\n",
    "\n",
    "# train, test, rm_features = fe_agg(train, test, rm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> processing uid...\n"
     ]
    }
   ],
   "source": [
    "def fe(train_df, test_df, rm_features):\n",
    "    \n",
    "    # fe with amount (user id)\n",
    "    train_df, test_df, rm_features, uid_list, cid_list = fe_uid(train_df, test_df, rm_features)\n",
    "    \n",
    "    # group aggregation\n",
    "    train_df, test_df, rm_features = fe_agg(train_df, test_df, rm_features, uid_list, cid_list)\n",
    "    \n",
    "    return train_df, test_df, rm_features\n",
    "\n",
    "train_df, test_df, remove_features = fe(train_df, test_df, remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape control:', train_df.shape, test_df.shape)\n",
    "print(remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Final features list\n",
    "features_columns = [col for col in list(train_df) if col not in remove_features]\n",
    "\n",
    "########################### Final Minification\n",
    "## I don't like this part as it changes float numbers\n",
    "## small change but change.\n",
    "## To be able to train lgbm without \n",
    "## minification we need to do some changes on model\n",
    "## we will do it later.\n",
    "if not LOCAL_TEST:\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    test_df  = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model Train\n",
    "if LOCAL_TEST:\n",
    "    lgb_params['learning_rate'] = 0.01\n",
    "    lgb_params['n_estimators'] = 10000\n",
    "    lgb_params['early_stopping_rounds'] = 100\n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=4)\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.01\n",
    "    lgb_params['n_estimators'] = 2000\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('../submissions/submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
